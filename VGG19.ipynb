{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, shutil, random, glob\n",
    "import bcolz\n",
    "import keras\n",
    "import keras.preprocessing.image\n",
    "from keras.layers import Input, Flatten, Dense, Dropout, Activation, BatchNormalization, GlobalMaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.models import Model\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Change directory data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = '/home/hai/Desktop/Cat&Dog/all/train/'\n",
    "f = '/home/hai/Desktop/Cat&Dog/train/'\n",
    "for img in os.listdir(TRAIN_DIR):\n",
    "    dogs_or_cats = 'dogs' if 'dog' in img else 'cats'\n",
    "    shutil.copy(TRAIN_DIR+img, f'train/{dogs_or_cats}/{img}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "train_data = gen.flow_from_directory('train', target_size=(224, 224), batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filenames = train_data.filenames\n",
    "bcolz.carray(train_filenames, rootdir='train_filenames', mode='w').flush()\n",
    "train_y = keras.utils.to_categorical(train_data.classes)\n",
    "bcolz.carray(train_y, rootdir='train_y', mode='w').flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG19(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(224, 224, 3),\n",
    "    pooling=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = base_model.predict_generator(train_data, steps=train_data.n)\n",
    "bcolz.carray(train_X, rootdir='train_X', mode='w').flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_ids = np.random.randint(25000, size=20000)\n",
    "val_ids = np.delete(np.arange(25000), trn_ids)\n",
    "\n",
    "trn_X = train_X[trn_ids, ...]\n",
    "trn_y = train_y[trn_ids]\n",
    "\n",
    "val_X = train_X[val_ids, ...]\n",
    "val_y = train_y[val_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(7, 7, 512))\n",
    "# x = keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2))(inputs)\n",
    "# x = Flatten()(x)\n",
    "# x = Dense(4096)(x)\n",
    "\n",
    "x = GlobalMaxPooling2D()(inputs)\n",
    "x = Dense(4096)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(2)(x)\n",
    "x = BatchNormalization()(x)\n",
    "predictions = Activation('softmax')(x)\n",
    "\n",
    "model = Model(inputs, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_2 (Glob (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4096)              2101248   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 8194      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 2)                 8         \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,125,834\n",
      "Trainable params: 2,117,638\n",
      "Non-trainable params: 8,196\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(Adam(lr=1e-4), 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 11260 samples\n",
      "Epoch 1/40\n",
      " - 5s - loss: 0.5848 - acc: 0.7046 - val_loss: 0.4819 - val_acc: 0.7438\n",
      "Epoch 2/40\n",
      " - 6s - loss: 0.3620 - acc: 0.8679 - val_loss: 0.3249 - val_acc: 0.8489\n",
      "Epoch 3/40\n",
      " - 6s - loss: 0.2846 - acc: 0.9243 - val_loss: 0.2325 - val_acc: 0.9144\n",
      "Epoch 4/40\n",
      " - 5s - loss: 0.2549 - acc: 0.9456 - val_loss: 0.1845 - val_acc: 0.9447\n",
      "Epoch 5/40\n",
      " - 5s - loss: 0.2403 - acc: 0.9538 - val_loss: 0.1603 - val_acc: 0.9605\n",
      "Epoch 6/40\n",
      " - 5s - loss: 0.2317 - acc: 0.9597 - val_loss: 0.1483 - val_acc: 0.9682\n",
      "Epoch 7/40\n",
      " - 5s - loss: 0.2261 - acc: 0.9623 - val_loss: 0.1428 - val_acc: 0.9710\n",
      "Epoch 8/40\n",
      " - 5s - loss: 0.2222 - acc: 0.9651 - val_loss: 0.1409 - val_acc: 0.9710\n",
      "Epoch 9/40\n",
      " - 5s - loss: 0.2193 - acc: 0.9668 - val_loss: 0.1410 - val_acc: 0.9708\n",
      "Epoch 10/40\n",
      " - 5s - loss: 0.2170 - acc: 0.9684 - val_loss: 0.1421 - val_acc: 0.9711\n",
      "Epoch 11/40\n",
      " - 5s - loss: 0.2151 - acc: 0.9696 - val_loss: 0.1437 - val_acc: 0.9710\n",
      "Epoch 12/40\n",
      " - 5s - loss: 0.2136 - acc: 0.9705 - val_loss: 0.1455 - val_acc: 0.9704\n",
      "Epoch 13/40\n",
      " - 5s - loss: 0.2122 - acc: 0.9707 - val_loss: 0.1474 - val_acc: 0.9704\n",
      "Epoch 14/40\n",
      " - 5s - loss: 0.2111 - acc: 0.9715 - val_loss: 0.1492 - val_acc: 0.9699\n",
      "Epoch 15/40\n",
      " - 5s - loss: 0.2100 - acc: 0.9714 - val_loss: 0.1508 - val_acc: 0.9701\n",
      "Epoch 16/40\n",
      " - 5s - loss: 0.2090 - acc: 0.9721 - val_loss: 0.1524 - val_acc: 0.9701\n",
      "Epoch 17/40\n",
      " - 6s - loss: 0.2081 - acc: 0.9732 - val_loss: 0.1538 - val_acc: 0.9701\n",
      "Epoch 18/40\n",
      " - 7s - loss: 0.2072 - acc: 0.9736 - val_loss: 0.1551 - val_acc: 0.9706\n",
      "Epoch 19/40\n",
      " - 5s - loss: 0.2064 - acc: 0.9740 - val_loss: 0.1563 - val_acc: 0.9714\n",
      "Epoch 20/40\n",
      " - 5s - loss: 0.2057 - acc: 0.9743 - val_loss: 0.1574 - val_acc: 0.9723\n",
      "Epoch 21/40\n",
      " - 5s - loss: 0.2049 - acc: 0.9747 - val_loss: 0.1585 - val_acc: 0.9732\n",
      "Epoch 22/40\n",
      " - 5s - loss: 0.2042 - acc: 0.9751 - val_loss: 0.1596 - val_acc: 0.9740\n",
      "Epoch 23/40\n",
      " - 5s - loss: 0.2035 - acc: 0.9757 - val_loss: 0.1607 - val_acc: 0.9749\n",
      "Epoch 24/40\n",
      " - 5s - loss: 0.2028 - acc: 0.9758 - val_loss: 0.1618 - val_acc: 0.9750\n",
      "Epoch 25/40\n",
      " - 5s - loss: 0.2022 - acc: 0.9759 - val_loss: 0.1629 - val_acc: 0.9758\n",
      "Epoch 26/40\n",
      " - 5s - loss: 0.2015 - acc: 0.9761 - val_loss: 0.1641 - val_acc: 0.9766\n",
      "Epoch 27/40\n",
      " - 5s - loss: 0.2009 - acc: 0.9765 - val_loss: 0.1653 - val_acc: 0.9773\n",
      "Epoch 28/40\n",
      " - 5s - loss: 0.2003 - acc: 0.9767 - val_loss: 0.1665 - val_acc: 0.9781\n",
      "Epoch 29/40\n",
      " - 5s - loss: 0.1998 - acc: 0.9768 - val_loss: 0.1678 - val_acc: 0.9784\n",
      "Epoch 30/40\n",
      " - 5s - loss: 0.1992 - acc: 0.9772 - val_loss: 0.1690 - val_acc: 0.9798\n",
      "Epoch 31/40\n",
      " - 6s - loss: 0.1987 - acc: 0.9778 - val_loss: 0.1703 - val_acc: 0.9801\n",
      "Epoch 32/40\n",
      " - 7s - loss: 0.1981 - acc: 0.9779 - val_loss: 0.1717 - val_acc: 0.9803\n",
      "Epoch 33/40\n",
      " - 7s - loss: 0.1976 - acc: 0.9779 - val_loss: 0.1730 - val_acc: 0.9806\n",
      "Epoch 34/40\n",
      " - 5s - loss: 0.1971 - acc: 0.9782 - val_loss: 0.1743 - val_acc: 0.9808\n",
      "Epoch 35/40\n",
      " - 5s - loss: 0.1966 - acc: 0.9786 - val_loss: 0.1756 - val_acc: 0.9808\n",
      "Epoch 36/40\n",
      " - 5s - loss: 0.1961 - acc: 0.9787 - val_loss: 0.1770 - val_acc: 0.9810\n",
      "Epoch 37/40\n",
      " - 6s - loss: 0.1957 - acc: 0.9787 - val_loss: 0.1783 - val_acc: 0.9812\n",
      "Epoch 38/40\n",
      " - 6s - loss: 0.1952 - acc: 0.9788 - val_loss: 0.1796 - val_acc: 0.9813\n",
      "Epoch 39/40\n",
      " - 6s - loss: 0.1947 - acc: 0.9790 - val_loss: 0.1809 - val_acc: 0.9812\n",
      "Epoch 40/40\n",
      " - 6s - loss: 0.1943 - acc: 0.9792 - val_loss: 0.1822 - val_acc: 0.9813\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1c38591940>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=trn_X, y=trn_y, batch_size=20000, epochs=40, validation_data=(val_X, val_y), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('VGG19_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('VGG19_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "TEST_DIR = '/home/hai/Desktop/Cat&Dog/all/test'\n",
    "def process_test_data():\n",
    "    testing_data = []\n",
    "    for img in os.listdir(TEST_DIR):\n",
    "        path = os.path.join(TEST_DIR,img)\n",
    "        img_num = img.split('.')[0]\n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.resize(img, (224,224))\n",
    "        testing_data.append([np.array(img), img_num])\n",
    "    #shuffle(testing_data)\n",
    "    #np.save('test_data.npy', testing_data)\n",
    "    return testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = process_test_data()\n",
    "with open('submission_file.csv','w') as f:\n",
    "    f.write('id,label\\n')\n",
    "            \n",
    "with open('submission_file.csv','a') as f:\n",
    "    for data in test_data:\n",
    "        img_num = data[1]\n",
    "        img_data = data[0]\n",
    "        orig = img_data\n",
    "        data = img_data.reshape(-1,7,7,512)\n",
    "        model_out = model.predict([data])[0]\n",
    "        f.write('{},{}\\n'.format(img_num,model_out[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
